# -*- coding: utf-8 -*-
"""airbnb_open_data_dag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2AQm4W_7rVzv9Nir7JzZRiRcIvG0WvN
"""

"""
DAG для анализа аренды Airbnb
Вариант задания №16

Автор: Нургалеева Гузель
Дата: 2025
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.utils.dates import days_ago
import math

# Конфигурация по умолчанию для DAG
default_args = {
    'owner': 'student',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# Создание DAG
dag = DAG(
    'airbnb_rent_analysis',
    default_args=default_args,
    description='Анализ аренды Airbnb - вариант 16',
    schedule_interval=timedelta(days=1),
    catchup=False,
    tags=['etl', 'airbnb', 'kaggle', 'variant_16']
)


def extract_from_kaggle(**context):
    """
    Extract: Скачивание данных об аренде Airbnb с Kaggle
    """
    import os
    import shutil
    import kagglehub

    print("Начинаем извлечение данных об аренде Airbnb с Kaggle...")

    DATA_DIR = '/opt/airflow/dags/data'
    os.makedirs(DATA_DIR, exist_ok=True)

    kaggle_json_path = '/home/airflow/.kaggle/kaggle.json'
    if not os.path.exists(kaggle_json_path):
        raise FileNotFoundError(f"Файл kaggle.json не найден в {kaggle_json_path}.")
    print(f"kaggle.json найден в: {kaggle_json_path}")

    dataset_name = "arianazmoudeh/airbnbopendata"
    print(f"Скачиваем датасет: {dataset_name}")

    path = kagglehub.dataset_download(dataset_name)
    print(f"Данные скачаны в: {path}")

    source_file = os.path.join(path, "airbnb.csv")
    dest_file = os.path.join(DATA_DIR, "airbnb.csv")
    shutil.copy2(source_file, dest_file)
    print(f"Файл скопирован в: {dest_file}")

    context['task_instance'].xcom_push(key='data_file_path', value=dest_file)
    print("Данные об арендах успешно извлечены с Kaggle")
    return dest_file

def load_raw_to_postgres(**context):
    """
    Load Raw: Загрузка сырых данных в PostgreSQL
    """
    import pandas as pd

    print("Начинаем загрузку сырых данных в PostgreSQL...")
    data_file_path = context['task_instance'].xcom_pull(key='data_file_path', task_ids='extract_from_kaggle')
    df = pd.read_csv(data_file_path)
    print(f"Загружено {len(df)} записей из файла")

    postgres_hook = PostgresHook(postgres_conn_id='analytics_postgres')

    column_mapping = {
        'neighbourhood group': 'neighbourhood_group', 'room type': 'room_type', 'price': 'price',
        'number of reviews': 'number_of_reviews', 'availability 365': 'availability_365'
    }

    df.rename(columns=column_mapping, inplace=True)

    allowed_columns = ['neighbourhood_group', 'room_type', 'price', 'number_of_reviews', 'availability_365']
    df_clean = df[[col for col in allowed_columns if col in df.columns]]

    postgres_hook.run("DROP TABLE IF EXISTS raw_airbnb;")
    create_table_sql = """
    CREATE TABLE raw_airbnb (
        id SERIAL PRIMARY KEY, neighbourhood_group TEXT, room_type TEXT, price INTEGER,
        number_of_reviews INTEGER, availability_365 INTEGER,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
    postgres_hook.run(create_table_sql)

    postgres_hook.insert_rows(
        table='raw_airbnb',
        rows=df_clean.values.tolist(),
        target_fields=list(df_clean.columns)
    )
    print(f"Успешно загружено {len(df_clean)} записей в raw_airbnb")

def transform_and_clean_data(**context):
    """
    Transform: Очистка данных, преобразование дат и обогащение для дашборда
    """
    import pandas as pd
    import math # Убедимся, что math импортирован

    print("Начинаем очистку, трансформацию и обогащение данных...")
    postgres_hook = PostgresHook(postgres_conn_id='analytics_postgres')

    df = postgres_hook.get_pandas_df("SELECT * FROM raw_airbnb;")

    import re
    # в ценах указан символ валюты, но т.к. он может быть разным, вместо удаления символа лучше извлечь сами цифры
    df['price'] = df['price'].astype(str)
    df['price'] = df['price'].replace(',', '', regex = True) # помимо символа валюты встречается ',', в случае с иностранными разделителями - это не отделение дробной части, лучше ее убрать
    df['price'] = [re.sub('\s+', '',i) for i in df['price']] # также лучше на всякий случай убрать пробелы, т.к. тип данных в столбце был строчным - они могут присутствовать
    df['price_clean'] = [re.findall(r'\d+', i) for i in df['price']] # находим и достаем последовательности цифр

    # т.к. re.findall() извлекает данные в список, в данном случае в список строчных данных, то нужно достать значения из списков.

    import numpy as np

    for i in range(len(df)):
        if isinstance(df['price_clean'][i], list) and len(df['price_clean'][i]) == 0:
            df.loc[i, 'price_clean'] = None
        elif isinstance(df['price_clean'][i], list):
            df.loc[i, 'price_clean'] = int(df['price_clean'][i][0])

    # пропуски в данных заполним: числовыне - медианой, строковые - словами "no_data"
    # нулевые значения в 'price' буду заполнять медианой в разбивке по 'neighbourhood group', 'room type', 'availability 365'.
    # 'availability 365' - большой разброс значений (438 вариантов), поэтому разобью на 10 категорий в допстолбце
    # в столбце 'availability 365' также встречаются отрицательны или нулевые значения - этот момент пока оставляю как есть, требует дополнительного изучения.

    df['availability_365_category'] = pd.qcut(df['availability_365'], 10, duplicates = 'drop')
    df['price_clean'] = df['price_clean'].fillna(
                         df.groupby(['neighbourhood_group', 'room_type','availability_365_category'])['price_clean'].
                         transform(
                         lambda x: x.median()
                         ),    )

    df['price'] = df['price_clean'].astype('int32')

    # пропуски в 'neighbourhood_group' заполним словами "no_data"
    df['neighbourhood_group'] =  df['neighbourhood_group'].fillna("no data")

    # 'number of reviews' можно тоже заполнить медианой, включив в группировку категорию цены.
    df['price_category'] = pd.qcut(df['price'], 10, duplicates = 'drop')
    df['number_of_reviews'] = df['number_of_reviews'].fillna(
                         df.groupby(['neighbourhood_group', 'room_type','availability_365_category', 'price_category'])['number_of_reviews'].
                         transform(
                         lambda x: x.median()
                         ),    )
    df['number_of_reviews'] = df['number_of_reviews'].astype('int32')

    # 'availability 365' пропуски заполню нолями
    df['availability_365'] =  df['availability_365'].fillna(0)

    #df = df.drop({'price_clean', 'availability_365_category', 'price_category'}, axis = 1)

    clean_columns = ['neighbourhood_group', 'room_type', 'price', 'number_of_reviews', 'availability_365']
    df_clean = df[clean_columns]

    # --- ИСПРАВЛЕНИЕ ЗДЕСЬ ---
    # Удаляем таблицу и все зависимые объекты (нашу VIEW)
    postgres_hook.run("DROP TABLE IF EXISTS stg_airbnb CASCADE;")

    create_staging_table_sql = """
    CREATE TABLE stg_airbnb (
        neighbourhood_group TEXT,
        room_type TEXT,
        price INTEGER,
        number_of_reviews INTEGER,
        availability_365 INTEGER
    );
    """
    postgres_hook.run(create_staging_table_sql)

    postgres_hook.insert_rows(
        table='stg_airbnb',
        rows=df_clean.values.tolist(),
        target_fields=list(df_clean.columns)
    )
    print(f"Успешно загружено {len(df_clean)} обогащенных записей в stg_airbnb")

# --- Задачи DAG ---

extract_task = PythonOperator(
    task_id='extract_from_kaggle',
    python_callable=extract_from_kaggle,
    dag=dag
)

load_raw_task = PythonOperator(
    task_id='load_raw_to_postgres',
    python_callable=load_raw_to_postgres,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform_and_clean_data',
    python_callable=transform_and_clean_data,
    dag=dag
)

create_datamart_task = PostgresOperator(
    task_id='create_datamart',
    postgres_conn_id='analytics_postgres',
    sql='datamart_variant_16.sql',
    dag=dag
)

# --- Определение зависимостей ---
extract_task >> load_raw_task >> transform_task >> create_datamart_task